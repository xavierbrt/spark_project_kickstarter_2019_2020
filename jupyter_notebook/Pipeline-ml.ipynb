{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.{DataFrame, SQLContext, SparkSession, Row}\n",
       "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}\n",
       "import org.apache.spark.ml.feature.{RegexTokenizer, StopWordsRemover, CountVectorizer, CountVectorizerModel, IDF, StringIndexer, OneHotEncoderEstimator, VectorAssembler}\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.linalg.Vector\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{DataFrame, SQLContext, SparkSession, Row}\n",
    "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}\n",
    "import org.apache.spark.ml.feature.{RegexTokenizer, StopWordsRemover, CountVectorizer, CountVectorizerModel, IDF, StringIndexer, OneHotEncoderEstimator, VectorAssembler}\n",
    "import org.apache.spark.ml.classification.{LogisticRegression}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration de SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@28103c6b\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conf = new SparkConf().setAll(Map(\n",
    "      \"spark.scheduler.mode\" -> \"FIFO\",\n",
    "      \"spark.speculation\" -> \"false\",\n",
    "      \"spark.reducer.maxSizeInFlight\" -> \"48m\",\n",
    "      \"spark.serializer\" -> \"org.apache.spark.serializer.KryoSerializer\",\n",
    "      \"spark.kryoserializer.buffer.max\" -> \"1g\",\n",
    "      \"spark.shuffle.file.buffer\" -> \"32k\",\n",
    "      \"spark.default.parallelism\" -> \"12\",\n",
    "      \"spark.sql.shuffle.partitions\" -> \"12\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@1eaaf461\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val spark = SparkSession\n",
    "      .builder\n",
    "      .config(conf)\n",
    "      .appName(\"TP Spark : Preprocessor\")\n",
    "      .getOrCreate()\n",
    "\n",
    "import spark.implicits._  // to use the symbol $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement du DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes: 107614\n",
      "Nombre de colonnes: 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 12 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df: DataFrame = spark\n",
    "            .read\n",
    "            .option(\"header\", true)\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .parquet(\"../data/prepared_trainingset/\")\n",
    "\n",
    "println(s\"Nombre de lignes: ${df.count}\")\n",
    "println(s\"Nombre de colonnes: ${df.columns.length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+------+--------------------+------------+--------+---------+-------------------+-------------------+-------------------+-------------+-----------+--------------------+\n",
      "|    project_id|                name|                desc|  goal|            keywords|final_status|country2|currency2|          deadline2|        created_at2|       launched_at2|days_campaign|hours_prepa|                text|\n",
      "+--------------+--------------------+--------------------+------+--------------------+------------+--------+---------+-------------------+-------------------+-------------------+-------------+-----------+--------------------+\n",
      "| kkst471421639|american options ...|looking to create...|100000|american-options-...|           0|      US|      USD|2014-11-15 17:31:27|2014-10-10 21:23:58|2014-10-16 17:31:27|           30|    140.125|american options ...|\n",
      "|kkst1098019088|iheadbones bone c...|wireless bluetoot...| 20000|iheadbones-bone-c...|           0|      US|      USD|2014-11-15 17:37:42|2012-08-30 23:07:05|2014-10-16 17:37:42|           30|   18642.51|iheadbones bone c...|\n",
      "|kkst1719475563| the fridge magazine|the fridge is a n...|   700| the-fridge-magazine|           0|      US|      USD|2014-11-15 17:41:58|2014-09-02 17:35:56|2014-09-16 17:41:58|           60|    336.101|the fridge magazi...|\n",
      "| kkst564469925|support new men's...|it s been over 10...| 12800|support-new-mens-...|           0|      US|      USD|2014-11-15 17:44:42|2014-09-07 19:32:20|2014-09-16 17:44:42|           60|    214.206|support new men's...|\n",
      "|kkst1213811673|             can('t)|a psychological h...|  1500|              cant-0|           0|      US|      USD|2014-11-15 17:57:32|2014-11-04 00:25:15|2014-11-05 17:57:32|           10|     41.538|can('t) a psychol...|\n",
      "+--------------+--------------------+--------------------+------+--------------------+------------+--------+---------+-------------------+-------------------+-------------------+-------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation des données textuelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_a15d3c67c72f\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Stage 1 : récupérer les mots des textes\n",
    "val tokenizer = new RegexTokenizer()\n",
    "  .setPattern(\"\\\\W+\")\n",
    "  .setGaps(true)\n",
    "  .setInputCol(\"text\")\n",
    "  .setOutputCol(\"tokens\")\n",
    "\n",
    "//val words = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopWordsRemover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_c5d8d93b2fca\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Stage 2 : retirer les stop words (liste : StopWordsRemover.loadDefaultStopWords(\"english\"))\n",
    "val stopWordsRemover = new StopWordsRemover()\n",
    "    .setInputCol(\"tokens\")\n",
    "    .setOutputCol(\"text_filtered\")\n",
    "\n",
    "//val words2 = stopWordsRemover.transform(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cvModel: org.apache.spark.ml.feature.CountVectorizer = cntVec_f916e3516fdf\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Stage 3 : computer la partie TF\n",
    "val cvModel: CountVectorizer = new CountVectorizer()//Model(Array(\"a\", \"b\", \"c\"))\n",
    "    .setInputCol(\"text_filtered\")\n",
    "    .setOutputCol(\"cv_features\")\n",
    "    //.setVocabSize(3)\n",
    "    //.setMinDF(2)\n",
    "\n",
    "//val words3 = cvModel.fit(words2).transform(words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idf: org.apache.spark.ml.feature.IDF = idf_5d5b944edbab\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Stage 4 : computer la partie IDF\n",
    "val idf = new IDF()\n",
    "    .setInputCol(\"cv_features\")\n",
    "    .setOutputCol(\"tfidf\")\n",
    "\n",
    "//val df2: DataFrame = idf.fit(words3).transform(words3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion des variables catégorielles en variables numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indexer_country: org.apache.spark.ml.feature.StringIndexer = strIdx_55fb5f18d035\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Stage 5 : convertir country2 en quantités numériques\n",
    "val indexer_country = new StringIndexer()\n",
    "    .setInputCol(\"country2\")\n",
    "    .setOutputCol(\"country_indexed\")\n",
    "    .setHandleInvalid(\"keep\")\n",
    "\n",
    "//val df4: DataFrame = indexer.fit(df3).transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indexer_currency: org.apache.spark.ml.feature.StringIndexer = strIdx_0587c4ba9e40\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Stage 6 : convertir currency2 en quantités numériques\n",
    "val indexer_currency = new StringIndexer()\n",
    "    .setInputCol(\"currency2\")\n",
    "    .setOutputCol(\"currency_indexed\")\n",
    "\n",
    "//val df5: DataFrame = indexer.fit(df4).transform(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_6ee3aed16785\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Stages 7 et 8: One-Hot encoder ces deux catégories\n",
    "val encoder = new OneHotEncoderEstimator()\n",
    "    .setInputCols(Array(\"country_indexed\", \"currency_indexed\"))\n",
    "    .setOutputCols(Array(\"country_onehot\", \"currency_onehot\"))\n",
    "\n",
    "//val df7: DataFrame = encoder.fit(df6).transform(df6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise données sous une forme utilisable par Spark.ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_4f750cb83f90\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"tfidf\", \"days_campaign\", \"hours_prepa\", \"goal\", \"country_onehot\", \"currency_onehot\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "//val df9 = assembler.transform(df8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du modèle de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_b27d1c2de6ca\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Stage 10 : créer/instancier le modèle de classification\n",
    "val lr = new LogisticRegression()\n",
    "  .setElasticNetParam(0.0)\n",
    "  .setFitIntercept(true)\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setLabelCol(\"final_status\")\n",
    "  .setStandardization(true)\n",
    "  .setPredictionCol(\"predictions\")\n",
    "  .setRawPredictionCol(\"raw_predictions\")\n",
    "  .setThresholds(Array(0.7, 0.3))\n",
    "  .setTol(1.0e-6)\n",
    "  .setMaxIter(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_a3b83b8d9334\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline = new Pipeline()\n",
    "    .setStages(Array(tokenizer, stopWordsRemover, \n",
    "                     cvModel, idf, indexer_country, indexer_currency,\n",
    "                     encoder, assembler, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement, test, et sauvegarde du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [project_id: string, name: string ... 12 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [project_id: string, name: string ... 12 more fields]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training, test) = df.randomSplit(Array(0.9, 0.1), seed=261)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_a3b83b8d9334\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "// On enregistre le modèle entraîné\n",
    "model.write.overwrite().save(\"../data/model/spark-logistic-regression-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfWithSimplePredictions: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 24 more fields]\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfWithSimplePredictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----+\n",
      "|final_status|predictions|count|\n",
      "+------------+-----------+-----+\n",
      "|           1|        0.0| 1812|\n",
      "|           0|        1.0| 2328|\n",
      "|           1|        1.0| 1590|\n",
      "|           0|        0.0| 5036|\n",
      "+------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithSimplePredictions.groupBy(\"final_status\", \"predictions\").count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_d5737757e5e0\n",
       "f1_score: Double = 0.6220287782615461\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "    .setLabelCol(\"final_status\")\n",
    "    .setPredictionCol(\"predictions\")\n",
    "    .setMetricName(\"f1\")\n",
    "val f1_score = evaluator.evaluate(dfWithSimplePredictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
